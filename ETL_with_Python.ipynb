{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a9d137-bd99-4505-80c7-1cee439b9a21",
   "metadata": {},
   "source": [
    "## [**Extract, Transform, Load (ETL) with Python**](#Extract,-Transform,-Load-(ETL)-with-Python)\n",
    "\n",
    "#### Practice with Pandas, Spark, SQlite and seaborn.\n",
    "\n",
    "Extract, Transform, Load or ETL is a three-phase process.\n",
    "ETL processing involves data extraction from sources, then processed by transformation such as cleaning, scrub, finally loaded into target.\n",
    "\n",
    "ETL or ELT  are just concepts. In fact, sometimes these 3 phases are combined flexibly. \n",
    "\n",
    "The following demos would practive these 3 phase in a flexible manner. And involve several popular tools such as Pandas, numpy, Spark/pySpark and SQLite.\n",
    "\n",
    "<img src=grafana.png width=\"50%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d79d4d-f5fa-4364-80be-1d7b6dd73366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sqlite3\n",
    "from collections import  Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e874b-3826-4683-aacc-db98294a8ae5",
   "metadata": {},
   "source": [
    "#### **Extract, Transform, Load or ETL** Demo 1\n",
    "\n",
    "Extract source data from local file, then transform the raw data into a new structure, followd by write cleaned data into a local DB. For convenience, our source data is quite small. \n",
    "\n",
    "* [ ] Data Extraction from sources.\n",
    "* [ ] Data Transformation such as cleaning, scrub.\n",
    "* [ ] Load data into target.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ea851-24c0-4b39-85e8-38a4551fab55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Extract Data**\n",
    "\n",
    "* [x] Data Extraction from sources;\n",
    "* [ ] Data Transformation such as cleaning, scrub.\n",
    "* [ ] Load data into target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc33d1-6259-4fc8-903a-a75d7b20a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data and basic path\n",
    "dataset = 'flight_ontime.csv'\n",
    "data_path = os.path.join(os.path.curdir, 'data', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd093980-2581-4914-9450-64d357f67714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List contents in data directory\n",
    "os.listdir(os.path.join(os.path.curdir, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedd826-42a5-4cdb-a097-daa03fa008a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the first 20 rows from source data, a csv file, by Pandas.\n",
    "df = pd.read_csv(data_path, nrows=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81199228-0de1-4460-86e4-86074df856be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm Pandas DataFrame data type.\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a96a9-0631-4708-8615-7d5b6aeab578",
   "metadata": {},
   "source": [
    "we extract the first bath of data from a local csv file, into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987071ac-75ca-4a55-8d27-927b78f45f2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Transform Data**\n",
    "\n",
    "* [x] Data Extraction from sources;\n",
    "* [x] Data Transformation such as cleaning, scrub.\n",
    "* [ ] Load data into target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c3929-4363-4918-be83-1a9c14845d95",
   "metadata": {},
   "source": [
    "Look at the high level info about the data first, by 2 Pandas functions info() and describe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e804749-d64c-4fa6-848d-9790289cad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check basic data type of each columns, non-null count, memory usage, etc.\n",
    "#No missing data in this DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086aee78-a540-4378-8d86-c47116fafa7a",
   "metadata": {},
   "source": [
    "20 entries in total, each one has 12 columns. No missing data, including 3 data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc48c09-6dec-47d9-8a22-96a82900d696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c4915-d884-4867-8663-d34904351d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default, only calculate numerical records  df.describe(include=[np.number])\n",
    "#df.describe(include='all') for both numerical and non-numerical.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b60dd6-b441-4950-ac02-070332bbc1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To gather statistical information from non-numerical columns.\n",
    "df.describe(exclude=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f55a31-1e55-4045-b852-f17b724fbf7e",
   "metadata": {},
   "source": [
    "By far we have a general idea about these data. Like airport 'ATL' ranked number one in frequency.The airplave 'N956DL' executed 2 flights. 'DL' is the only unique in the 'OP_CARRIER' column. What's the second airport in frequency? How many short-haul or long-haul flights are operated? What columns are we interested in? To get answers, we need Pandas or Python standard libs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd2b33-bb89-476c-8715-7df11ef54997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1 call Python standard lib.  from collections import  Counter\n",
    "# Counter(df.ORIGIN).most_common(2)[1]\n",
    "Counter(df.ORIGIN).most_common()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640fecec-359b-47de-9047-443ebfa146ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df.DEST).most_common( )[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f8cb3-88de-44f9-9638-c5bcb86c0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2 call Pandas.\n",
    "pd.DataFrame.value_counts(df[['ORIGIN']])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275f30d-e7fa-43d1-b90c-ad2f533824c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.value_counts(df[['DEST']])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd9e9f-fb7e-4c68-9ca2-0643621f1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3 call Pandas.\n",
    "# df.ORIGIN.agg(lambda x: x.value_counts())\n",
    "df['ORIGIN'].agg(lambda x: x.value_counts())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad05d3c-e587-422c-9d8b-f4eb9af39070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DEST'].agg(lambda x: x.value_counts())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caff55f-b994-4ee1-948b-e3b1ec6d3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4 call Pandas.\n",
    "df.groupby(['ORIGIN'])['ORIGIN'].agg('count').sort_values(ascending=False, inplace=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a50832-18bd-462b-9bcd-fa162fd2b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['DEST'])['DEST'].agg('count').sort_values(ascending=False, inplace=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757a871-29ff-43ee-9d06-7eef11ede04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Or even plot.\n",
    "\n",
    "sns.displot(df, x=\"ORIGIN\", hue=\"ORIGIN\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114983bb-9bab-46cc-8e77-6129c812866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df, x=\"ORIGIN\", kind=\"count\", palette=\"ch:.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2792b-66b8-43e7-b15f-6b6011d58478",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"DEST\", hue=\"DEST\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cee3e5-2689-4bd0-a78b-000a38a25d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df, x=\"DEST\", kind=\"count\", palette=\"ch:.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88407b8-e2b3-43fc-a04a-1e3ffccb7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFO rank No2 as original airport\n",
    "\n",
    "df[df['ORIGIN'] == 'SFO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cdbb90-e199-4f5b-b2b1-dd34fb2e82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSP rank No2 as destination.\n",
    "df[df['DEST'] == 'MSP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b68c6-000d-4815-b173-2e52fd4da340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce92e576-0472-4e3b-83b7-a2fdece102cc",
   "metadata": {},
   "source": [
    "According the basic aggregation, the flight DL1859 is quite close to long-haul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ed80c-fe99-453e-8ce0-9ed6206dd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['DISTANCE']].agg(['max','min','mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941df926-987f-4aaa-adeb-71406933ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['DISTANCE']].apply(['max','min','mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c940d7-3ae5-4803-83da-8e95cf7398ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DISTANCE'].sort_values(ascending=False, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefe1c6-9046-4afe-9aab-9d536d7d14a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca284ef2-489b-46b6-b1d5-13013f77efb4",
   "metadata": {},
   "source": [
    "Plot distribution, based on distance as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865bae78-9036-404f-b01a-c0c77157df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DISTANCE'].sort_values().plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ba9fc-f17c-4d20-978a-37e2e3baef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"DISTANCE\", binwidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e8246-6f1a-46d6-92e2-00b61bccebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"DISTANCE\", hue=\"DEST\",  element=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d6355-0414-426c-8067-5a96095d717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"DISTANCE\", hue=\"DEST\",   multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2720d-ae4e-4cd4-98e5-cf1393cbbdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['OP_CARRIER'])['OP_CARRIER'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37764c46-321d-41db-a058-5a329eaa45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['DEST'])['DEST'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a601634-a93b-4764-bde0-c31455ee43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb447f4c-b600-4aa1-862e-794a348e0d5e",
   "metadata": {},
   "source": [
    "By far, the first 10 columns looks ok. So drop the last 2 columns. We also have several solutions to select desired columns or drop not needed columns. The first of following solutions goes straightforward. Should be familiar with 'axis' parameter to apply the second one.\n",
    "The last one is the most tedious among them, since we keep most columns. For the first 2 solutions, we could use inplace=True as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c85a1-7796-4b54-b331-941d785498cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: straightforward approach\n",
    "# \n",
    "df = df.drop(columns=['ARR_DELAY', 'ARR_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39075066-7139-4c52-90cb-664847ee8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: straightforward approach\n",
    "# df = df.drop(['ARR_DELAY', 'ARR_DEL15'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af391bc-7be4-4897-834d-6059c8311d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: DataFrame column selection. Create a  20 x 10 DataFrame. \n",
    "# df = df[['FL_DATE', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN', \n",
    "#          'ORIGIN_CITY_NAME', 'DEST', 'DEST_CITY_NAME', 'DISTANCE', 'AIR_TIME',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81b7f4-6ad0-42fb-9cf9-90b676bdbef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845fecc6-0cf4-4094-8a35-f1589b5863ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -R  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79262d7a-e2e9-436b-a370-96ac2c8b058e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dace0ee-838b-4b95-a5a1-5e0dbe66d27b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Load**\n",
    "\n",
    "* [x] Data Extraction from sources;\n",
    "* [x] Data Transformation such as cleaning, scrub.\n",
    "* [x] Load data into target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa07a4-e1c7-40d5-9ccf-6f23f9983ca5",
   "metadata": {},
   "source": [
    "Load our new DataFrame dfn into a SQLite file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898bd0d2-91e0-4f08-9055-e96ac9e9ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite3 create db and table\n",
    "\n",
    "db_filename='flight10.db'\n",
    "\n",
    "SQL = '''CREATE TABLE domestic20 (FL_DATE text, OP_CARRIER text, TAIL_NUM text, OP_CARRIER_FL_NUM int, ORIGIN text, ORIGIN_CITY_NAME text, DEST text, DEST_CITY_NAME text, AIR_TIME real, DISTANCE real)'''\n",
    "\n",
    "with sqlite3.connect(db_filename) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    # Create table 'domestic20'\n",
    "    cursor.execute(SQL)\n",
    "    conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1acfa-41a9-4027-ba12-fd3d60187536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with sqlit3\n",
    "'''\n",
    "SQL = \"\"\"\n",
    "insert into domestic20 (FL_DATE, OP_CARRIER, TAIL_NUM, OP_CARRIER_FL_NUM, ORIGIN, ORIGIN_CITY_NAME, DEST, DEST_CITY_NAME, AIR_TIME, DISTANCE)\n",
    "values (:FL_DATE, :OP_CARRIER, :TAIL_NUM, :OP_CARRIER_FL_NUM, :ORIGIN, 'ORIGIN_CITY_NAME', :DEST, 'DEST_CITY_NAME', :AIR_TIME, :DISTANCE)\n",
    "\"\"\"\n",
    "\n",
    "For demo we use following instead.\n",
    "'''\n",
    "\n",
    "SQL = \"INSERT INTO  domestic20   VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "\n",
    "with sqlite3.connect(db_filename) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany(SQL, df.values)\n",
    "    conn.commit()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5104e8-d57f-42ff-9aad-48d6b7cbb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e70df6-7eba-424e-8596-073db07cb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with sqlit3\n",
    "\n",
    "with sqlite3.connect(db_filename) as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "    select * from domestic20\n",
    "    \"\"\")\n",
    "    print('\\n20 flights:')\n",
    "    for row in cursor.fetchmany(20):\n",
    "        FL_DATE, OP_CARRIER , TAIL_NUM, OP_CARRIER_FL_NUM, \\\n",
    "        ORIGIN, ORIGIN_CITY_NAME, DEST, DEST_CITY_NAME, \\\n",
    "        AIR_TIME, DISTANCE = row\n",
    "        \n",
    "        print('{:<10} {:<4} {:<8} {:<4} {:<5} {:<20} {:<5} {:<20} {:<10} {:<10}'.format(\n",
    "            FL_DATE, OP_CARRIER , TAIL_NUM, OP_CARRIER_FL_NUM, \\\n",
    "            ORIGIN, ORIGIN_CITY_NAME, DEST, DEST_CITY_NAME, \\\n",
    "            AIR_TIME, DISTANCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b0158-be78-453c-a8cc-6bb2dfe07555",
   "metadata": {},
   "source": [
    "The first round ETL process is finished. We have practiced some basic functions of Pandas and SQLite. Next we would practice pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58cfd1-33b0-40c7-9e0b-9c3b7f2bd9cc",
   "metadata": {},
   "source": [
    "#### **Extract, Transform, Load or ETL** Demo 2\n",
    "\n",
    "* [ ] Data Extraction from sources.\n",
    "* [ ] Data Transformation such as cleaning, scrub.\n",
    "* [ ] Load data into target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a827991-108d-45b2-891d-9d3034ad3464",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Extract Data**\n",
    "\n",
    "* [x] Data Extraction from sources;\n",
    "* [ ] Data Transformation such as cleaning, scrub.\n",
    "* [ ] Load data into target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bca522-708d-49e0-afca-4a2e16f219b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession for further operation \n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce710987-9367-4f90-b784-2eb89ce88806",
   "metadata": {},
   "source": [
    "Access our SQLite db by pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150116e-8858-4132-81ae-1737da36caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark read db\n",
    "driver = \"org.sqlite.JDBC\"\n",
    "url = \"jdbc:sqlite:\" + db_filename\n",
    "tablename = \"domestic20\"\n",
    "\n",
    "spark.read.format('jdbc').options(driver=driver, \n",
    "                                  dbtable=tablename,\n",
    "                                  url=url).load().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7aa55a-3557-43e0-92da-d7edada7ba95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7147c5-dccb-42d3-86e8-e729a278d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark read db\n",
    "# Attention: Here use option() not options().\n",
    "# Alternative of above method\n",
    "\n",
    "dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\\\n",
    "                                       .option(\"dbtable\", tablename)\\\n",
    "                                       .option(\"driver\",  driver)\\\n",
    "                                       .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c0654-0207-4ad8-944c-def902fc9d51",
   "metadata": {},
   "source": [
    "Above operation created a 'pyspark.sql.dataframe.DataFrame' from our SQLite DB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fcc8c7-0838-449d-9932-c1a17f5bda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dbDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa05ef-37ed-469c-9ed6-40da9f0ec5a8",
   "metadata": {},
   "source": [
    "List all entries of our SQLite DB. Since it's so tiny, we could do it without concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3867aa-7f87-40e8-9dc4-68913876ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbDataFrame.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d0de9-0c02-4840-a0b6-fd0489beca9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Transform Data**\n",
    "\n",
    "* [x] Data Extraction from sources;\n",
    "* [x] Data Transformation such as cleaning, scrub.\n",
    "* [ ] Load data into target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844d7a5-4c2e-4007-a9c6-102235cc6edf",
   "metadata": {},
   "source": [
    "This time we create our own function to transform DataFrame. Define a function to convert 'nmi' into 'km'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5216b73-ce47-4250-99a3-1b40dba60547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf convert nmi to km for DISTANCE\n",
    "def nmi_km(nmi):\n",
    "    return nmi * 1.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2d814-5a6d-4374-805a-1b0dc1be2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our own function\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "convert_distance = udf(nmi_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1ab78-5f6e-4c01-b074-87d86ae9e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbDataFrame.select(convert_distance(col('DISTANCE'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea06f1-b580-4137-9e70-3e8e21c4266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf convert distance from nmi to km\n",
    "dbDataFrame.withColumn('DISTANCE_km', convert_distance(dbDataFrame.DISTANCE)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdcbcb-3423-44f0-8852-ce89f433c5c2",
   "metadata": {},
   "source": [
    "Function nmi_km() works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0fe95-9467-4a31-9c46-1d7073cb5d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1523f-3e0f-492a-919e-7e802356323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680e57c-3954-4ae9-a191-d43046c42c83",
   "metadata": {},
   "source": [
    "pyspark also includes many useful tools for string anf number operations. Let's try some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94eaffd-a89e-49c6-bbda-a3fc25234b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#String operation\n",
    "from pyspark.sql.functions import upper, lower\n",
    "\n",
    "dbDataFrame.withColumn('lower_ORIGIN', lower(dbDataFrame.ORIGIN)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68be715-9c51-4b4e-a31f-c1ae76a3d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregation \n",
    "from pyspark.sql.functions import count\n",
    "dbDataFrame.select(count(\"DEST\")).show() # 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace2dd4-4186-46c5-b73a-77de6e012674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import countDistinct\n",
    "dbDataFrame.select(countDistinct(\"DEST_CITY_NAME\")).show() # 11\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT COUNT(DEST_CITY_NAME *) FROM TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d18ca8-2374-4f63-b498-31b71fa926ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "dbDataFrame.select(approx_count_distinct(\"DEST_CITY_NAME\", 0.2)).show() # \n",
    "\n",
    "# -- in SQL\n",
    "# SELECT approx_count_distinct(DEST_CITY_NAME, 0.2) FROM TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9365a7-6033-472e-bc2b-9cb8285103e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import first, last\n",
    "dbDataFrame.select(first(\"DEST_CITY_NAME\"), last(\"DEST_CITY_NAME\")).show()\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT first(DEST_CITY_NAME), last(DEST_CITY_NAME) FROM TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0f03f-a3e4-4cc5-afa0-79a80321d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in Python\n",
    "from pyspark.sql.functions import min, max\n",
    "dbDataFrame.select(min(\"DISTANCE\"), max(\"DISTANCE\")).show()\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT min(DISTANCE), max(DISTANCE) FROM TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4fdee-69ba-48db-b07a-8d25de69cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in Python\n",
    "from pyspark.sql.functions import sum\n",
    "dbDataFrame.select(sum(\"DISTANCE\")).show() # 2711\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT sum(DISTANCE) FROM TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0486133-a048-4ec0-9f1e-b3e73af04a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in Python\n",
    "from pyspark.sql.functions import sumDistinct, sum_distinct\n",
    "dbDataFrame.select(sum_distinct(\"AIR_TIME\")).show() # \n",
    "\n",
    "# \n",
    "# -- in SQL\n",
    "# SELECT SUM(AIR_TIME) FROM TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3f014-3d39-4b03-98e7-90d09a848a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in Python\n",
    "from pyspark.sql.functions import sum, count, avg, expr, max, min\n",
    "\n",
    "\n",
    "dbDataFrame.select(\n",
    "    count(\"DISTANCE\").alias(\"dest_route\"),\n",
    "    sum(\"DISTANCE\").alias(\"total_distance\"),\n",
    "    avg(\"DISTANCE\").alias(\"avg_distance\"),\n",
    "    expr(\"mean(DISTANCE)\").alias(\"mean_distance\"),\n",
    "    max(\"DISTANCE\").alias(\"long_route\"),\n",
    "    min(\"DISTANCE\").alias(\"short_route\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_distance/dest_route\",\n",
    "    \"avg_distance\",\n",
    "    \"mean_distance\", \"short_route\" , \"long_route\").show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbb55b-da62-44c6-99c8-f64563de18ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b95b8fe3-e069-4070-b9cb-2fe4195a8bd0",
   "metadata": {},
   "source": [
    "Similar to Pandas operation we just did amoment ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c84c0-0a0d-471a-876d-9ec6094a933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping\n",
    "dbDataFrame.groupBy(\"ORIGIN\", \"DEST\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cd9ed-e1c5-4bef-aa4b-35953e43f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping\n",
    "dbDataFrame.groupBy(\"ORIGIN\",).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b579493-2a25-4af6-b520-dad1aee074a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbDataFrame.groupBy(\"DEST\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19e71b-9b41-444b-94e3-f9812f3df51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "dbDataFrame.groupBy(\"DEST\").agg(\n",
    "    count(\"ORIGIN\").alias(\"original\"),\n",
    "    expr(\"count(ORIGIN)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed980a-af39-45ec-ba07-ac4dd8dd53fd",
   "metadata": {},
   "source": [
    "#### **Extract, Transform, Load or ETL** Demo 3\n",
    "\n",
    "This section mainly focus on pyspark read write SQLite DB.\n",
    "\n",
    "* [ ] Try pyspark.pandas.\n",
    "* [ ] Read/Write data with pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c2d54-7e96-4023-8245-8bb9b3afe614",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Try pyspark.pandas**\n",
    "\n",
    "* [x] Try pyspark.pandas.\n",
    "* [ ] Read/Write data with pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648d65d-8c4b-4031-83a2-9e17e6e351eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark.pandas as ps\n",
    "\n",
    "df = ps.read_sql(\"domestic20\", con=\"jdbc:sqlite:{}/{}\".format(os.getcwd(), db_filename  ))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467baa47-5229-493d-acf8-b5f5c756e603",
   "metadata": {},
   "source": [
    "This is not **pandas.core.frame.DataFrame** but a **pyspark.pandas.frame.DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3378a-d63d-4e0e-aea7-bfd46b386d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f96021-a0dc-4ef3-909d-6303c26e181a",
   "metadata": {},
   "source": [
    "Select a subset of the new DataFrame by ps.read_sql. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e07d07-c7ac-46ac-b3f6-fa281e5e62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ps.read_sql(\"SELECT FL_DATE, OP_CARRIER, TAIL_NUM, OP_CARRIER_FL_NUM, ORIGIN, DEST, AIR_TIME, DISTANCE FROM  domestic20 LIMIT 5\", \\\n",
    "                  con=\"jdbc:sqlite:{}/{}\".format(os.getcwd(), db_filename ))\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e58d5b-dd11-45d7-8735-b8045dc66845",
   "metadata": {},
   "source": [
    "Another approach to select data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfca477-e321-4290-9b5f-7579fb10d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = ps.read_sql(\"domestic20\", \n",
    "                  con=\"jdbc:sqlite:{}/{}\".format(os.getcwd(), db_filename),\n",
    "                  columns=['ORIGIN', 'ORIGIN_CITY_NAME', 'DEST', 'DEST_CITY_NAME', 'DISTANCE']\n",
    "                 )\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6beac0-1db0-4ecd-ba64-e4e7d9ae8739",
   "metadata": {},
   "source": [
    "**Create a PySpark DataFrame from a pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723fd2e-a3f5-4abb-a313-5a8328525419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame\n",
    "df = pd.read_csv(data_path, nrows=20)\n",
    "# DataFrame column selection. Create a new  20 x 10 DataFrame\n",
    "col10_20 = df[['FL_DATE', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', \n",
    "               'ORIGIN', 'ORIGIN_CITY_NAME','DEST', 'DEST_CITY_NAME', \n",
    "               'AIR_TIME', 'DISTANCE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeda75d-ac23-4668-a4f4-b2dc22b128a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(col10_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489b933-b83f-40c7-a4fb-9306597ecd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(col10_20)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76884fc7-8575-4eec-bd58-3657aed51d11",
   "metadata": {},
   "source": [
    "**pyspark.sql.dataframe.DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ed763-4673-49f6-8177-48d1fd6623f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb3748-9160-4eca-9b1f-1b06b3443d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be647e2-b484-47a5-b11f-a80b30271b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da74dc-2a20-4ef9-aeec-19c906a8fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af0dab9-f02f-4bf4-9ac5-cfabc27d5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc948a-a7b1-4ec3-a1f2-d6cc9665ea0f",
   "metadata": {},
   "source": [
    "**convert pyspark.sql.dataframe.DataFrame to pandas.core.frame.DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c919a3-a24a-470d-942b-015b31af8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "type(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44d975-ebcc-4f0b-a048-540d89c3a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ba70c-7d9a-461c-92e2-e5450c3387ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c31e3-16c2-489b-81b7-2d719b66cbcf",
   "metadata": {},
   "source": [
    "Transform data with new type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6aa498-49c0-4c19-8ee1-76cda7219164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.ORIGIN_CITY_NAME).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa13be2-1f58-494e-9392-ec1ffda185cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('lower_ORIGIN', lower(df.ORIGIN)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764dd8a8-2a42-4d16-8f38-14a5e40b425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.DISTANCE >= 1000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0de064-875f-4001-bf68-8e5c0fe4afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ORIGIN', 'DEST', 'AIR_TIME','DISTANCE' ].groupby('ORIGIN', 'DEST').avg('AIR_TIME','DISTANCE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14421d-b585-4e3d-8295-96c8312fc13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d723f6-728d-41ae-b81a-a85527127a4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Read/Write data with pyspark.**\n",
    "\n",
    "* [x] Try pyspark.pandas.\n",
    "* [x] Read/Write data with pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85572fd3-f6a5-43ba-89b5-40a970db526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark DataFrame write into db. Attention the mode\n",
    "# mode = \"append\"\n",
    "# mode = \"overwrite\"\n",
    "# mode = \"ignore\"\n",
    "\n",
    "# mode : str, optional\n",
    "#     specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "#     * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
    "#     * ``overwrite``: Overwrite existing data.\n",
    "#     * ``ignore``: Silently ignore this operation if data already exists.\n",
    "#     * ``error`` or ``errorifexists`` (default case): Throw an exception if data already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4bfeb0-b2b1-4823-af80-c0916d793b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = \"org.sqlite.JDBC\"\n",
    "# url = ''.join((\"jdbc:sqlite:\", db_filename))\n",
    "# tablename = \"domestic20\"\n",
    "# \n",
    "mode = \"ignore\"\n",
    "# \n",
    "df.write.jdbc(url, tablename, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73238b65-f51b-4910-b8cd-8641a701d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('jdbc').options(driver='org.sqlite.JDBC', \n",
    "                                  dbtable=tablename,\n",
    "                                  url=url).load().show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a25db-cf1e-46ce-bb27-92e3dd32a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0787761-8a6d-4b89-9b43-c33841f85c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset\n",
    "df1000 = df.filter(df.DISTANCE >= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66668ec5-17f7-4eed-83bb-475c5405482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df1000.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd668f30-2a84-4d6b-80f6-1b32398b4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "type(df1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87135e-a7b2-4405-b690-f8a6c94691cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df1000.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a2dd2-875e-484e-9207-bdfb5c95ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"AIR_TIME\" and \"DISTANCE\" are highly correlated.\n",
    "df1000.stat.corr(\"AIR_TIME\", \"DISTANCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99ff7d-73b2-4c6e-85a9-fc955a0e41b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the table domestic20 of database will be overwrite by following operation.\n",
    "# New table will only has DISTANCE >= 1000 entries of the former one.\n",
    "# \n",
    "mode = \"overwrite\"\n",
    "# \n",
    "df1000.write.jdbc(url, tablename, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780e065-077e-4a72-924d-5cca64e5b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('jdbc').options(driver='org.sqlite.JDBC', \n",
    "                                  dbtable=tablename,\n",
    "                                  url=url).load().show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553d99a-728e-427a-968b-992f1cc4ae64",
   "metadata": {},
   "source": [
    "overwrite mode replace the SQLite with news data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541c5f6-45ec-4dc0-ab87-3f77863f0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further data selectino from pyspark.sql.dataframe.DataFrame\n",
    "df.filter(df.DISTANCE <= 500).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90fbdc-fe53-4d11-907e-064ac4e378d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df500 = df.filter(df.DISTANCE <= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3eebce-63c3-4143-909d-c023ce3f6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New table will add DISTANCE <= 500 entries of the former one.\n",
    "# \n",
    "mode = \"append\"\n",
    "# \n",
    "df500.write.jdbc(url, tablename, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cccc763-39f2-498e-afc1-0d5a02314885",
   "metadata": {},
   "source": [
    "New data is appended into SQLite DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65c601-2a22-41ac-9c3a-773621004a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm short distance flights \"DISTANCE <= 300\"  had been added successfully.\n",
    "# mode = \"append\"\n",
    "spark.read.format('jdbc').options(driver='org.sqlite.JDBC', \n",
    "                                  dbtable=tablename,\n",
    "                                  url=url).load().show(60, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3f6ac-08fb-4805-a23d-d00c6bf12703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d7b81b2-88e2-4eae-8b18-ba797969c39e",
   "metadata": {},
   "source": [
    "### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153da11-fcd7-4d17-b445-da9db0e88422",
   "metadata": {},
   "source": [
    "From the 3 demos we get familiar wth ETL and related tools such as Pandas and pyspark. These are just small part for real cases. By work through above steps, audience could build their own ETL/ELT demo and handle basic projects. This notebook is not the final version. I would improve it gradually. \n",
    "\n",
    "The picture at the very top of this note is a grafana dashboard. I will add grafana into this note.\n",
    "\n",
    "Hope this note could help someone. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa1264-efc3-4878-a869-298f30dad772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61781751-86c3-45a4-bcc9-2bd5cc66e71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
